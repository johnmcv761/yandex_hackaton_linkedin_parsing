{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Исследование по теме наставничества. Часть I - Парсер"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Информация о компании**\\\n",
    "Отрасль и направления деятельности: EdTech, сервис онлайн-образования\n",
    "\n",
    "**Общее описание задачи**\\\n",
    "Провести исследование по теме настаничества и менторства на основании контента социальной сети Linkedin, размещенного в открытом доступе, созданнного целевой аудиторией\n",
    "\n",
    "**Цели исследования**\n",
    "- Определить топ-10 тем в направлении наставничества на основании наибольшего охвата, используя теги `Наставничество`, `менторство`, `коучинг`, `mentorship`, `mentor`, `coaching`, `buddy`.\n",
    "- Определить топ-10 тем по просмотрам, реакциям: лайкам, комментариям, репостам среди IT-специалистов, подходящих под описание целевой аудитории исследования\n",
    "- Дополнить профили целевой аудитории новыми параметрами\n",
    "\n",
    "**Требования к результату**\n",
    "- Собранный датасет в виде CSV или JSON файла (не ссылки)\n",
    "- Презентация в виде ссылки на Google Slides\n",
    "- Ссылка на код проекта размещенного на GitHub"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.2)\n",
      "Requirement already satisfied: numpy==1.24.3 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: matplotlib in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (3.7.1)\n",
      "Requirement already satisfied: seaborn==0.11.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (0.11.1)\n",
      "Requirement already satisfied: plotly==5.15.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (5.15.0)\n",
      "Requirement already satisfied: bs4 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.0.1)\n",
      "Requirement already satisfied: requests==2.31.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (2.31.0)\n",
      "Requirement already satisfied: lxml==4.9.2 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (4.9.2)\n",
      "Requirement already satisfied: undetected-chromedriver==3.5.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (3.5.0)\n",
      "Requirement already satisfied: openpyxl==3.1.2 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (3.1.2)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (5.9.0)\n",
      "Requirement already satisfied: scipy>=1.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from seaborn==0.11.1->-r requirements.txt (line 4)) (1.10.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from plotly==5.15.0->-r requirements.txt (line 5)) (8.2.2)\n",
      "Requirement already satisfied: packaging in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from plotly==5.15.0->-r requirements.txt (line 5)) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from requests==2.31.0->-r requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from requests==2.31.0->-r requirements.txt (line 7)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from requests==2.31.0->-r requirements.txt (line 7)) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from requests==2.31.0->-r requirements.txt (line 7)) (2023.5.7)\n",
      "Requirement already satisfied: selenium>=4.9.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (4.10.0)\n",
      "Requirement already satisfied: websockets in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (11.0.3)\n",
      "Requirement already satisfied: et-xmlfile in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from openpyxl==3.1.2->-r requirements.txt (line 10)) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 3)) (3.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from bs4->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: fastjsonschema in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from nbformat>=4.2.0->-r requirements.txt (line 11)) (2.17.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from nbformat>=4.2.0->-r requirements.txt (line 11)) (4.17.3)\n",
      "Requirement already satisfied: jupyter-core in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from nbformat>=4.2.0->-r requirements.txt (line 11)) (5.3.1)\n",
      "Requirement already satisfied: traitlets>=5.1 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from nbformat>=4.2.0->-r requirements.txt (line 11)) (5.9.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->-r requirements.txt (line 11)) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from jsonschema>=2.6->nbformat>=4.2.0->-r requirements.txt (line 11)) (0.19.3)\n",
      "Requirement already satisfied: six>=1.5 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: trio~=0.17 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (0.22.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (0.10.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->-r requirements.txt (line 11)) (3.7.0)\n",
      "Requirement already satisfied: pywin32>=300 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->-r requirements.txt (line 11)) (306)\n",
      "Requirement already satisfied: sortedcontainers in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.10)\n",
      "Requirement already satisfied: outcome in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: sniffio in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from urllib3<3,>=1.21.1->requests==2.31.0->-r requirements.txt (line 7)) (1.7.1)\n",
      "Requirement already satisfied: pycparser in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in e:\\gitlocal\\linkedin_parsing\\.venv\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver==3.5.0->-r requirements.txt (line 9)) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import base64\n",
    "import lxml\n",
    "\n",
    "import undetected_chromedriver as uc\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим параметры для браузера парсера, возьмем логин и пароль для Linkedin из `licreds.txt` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0'}\n",
    "\n",
    "# login password saved in licreds.txt\n",
    "with open(\"licreds.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    USER_LOGIN, USER_PASSWORD = lines[0], lines[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим функции для сбора общей информации из профиля, попутно собираем немного сырой информации(вдруг пригодится), и посты с реакциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_profile_info(driver, profile_url):\n",
    "    time.sleep(random.uniform(3,5))\n",
    "\n",
    "    driver.get(profile_url)        # this will open the link\n",
    "\n",
    "    # Extracting data from page with BeautifulSoup\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml') # 'lxml'  \"html.parser\"\n",
    "\n",
    "    # Extracting the HTML of the complete introduction box\n",
    "    # that contains the name, company name, and the location\n",
    "    intro = soup.find('div', {'class': 'pv-text-details__left-panel'})\n",
    "    \n",
    "    # collect some additional raw info\n",
    "    expa = soup.findAll('span', {'class': 'visually-hidden'})\n",
    "    experience_list = [x.text for x in expa[:]]\n",
    "\n",
    "    # In case of an error, try changing the tags used here.\n",
    "    name_loc = intro.find(\"h1\")\n",
    "\n",
    "    # Extracting the Name\n",
    "    name = name_loc.get_text().strip()\n",
    "    # strip() is used to remove any extra blank spaces\n",
    "\n",
    "    works_at_loc = intro.find(\"div\", {'class': 'text-body-medium'})\n",
    "\n",
    "    # this gives us the HTML of the tag in which the Company Name is present\n",
    "    # Extracting the Company Name\n",
    "    works_at = works_at_loc.get_text().strip()\n",
    "\n",
    "    POSTS_URL_SUFFIX = 'recent-activity/all/'\n",
    "\n",
    "    time.sleep(random.uniform(3,7))\n",
    "\n",
    "    # Get current url from browser\n",
    "    cur_profile_url = driver.current_url\n",
    "\n",
    "    # Parse posts\n",
    "    posts = get_and_print_user_posts(driver, cur_profile_url + POSTS_URL_SUFFIX)\n",
    "    \n",
    "    return cur_profile_url, name, works_at, experience_list, posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_print_user_posts(driver, posts_url):\n",
    "    driver.get(posts_url)\n",
    "\n",
    "    #Simulate scrolling to capture all posts\n",
    "    SCROLL_PAUSE_TIME = random.uniform(3,7)\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # We can adjust this number to get more posts\n",
    "    NUM_SCROLLS = random.randint(6,10)\n",
    "\n",
    "    for i in range(NUM_SCROLLS):\n",
    "        time.sleep(random.uniform(4,6))\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Parsing posts\n",
    "    src = driver.page_source\n",
    "\n",
    "    # Now using beautiful soup\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    # soup.prettify()\n",
    "\n",
    "    posts = soup.find_all('li', class_='profile-creator-shared-feed-update__container')\n",
    "\n",
    "    all_posts_with_react = []\n",
    "\n",
    "    for post_src in posts:\n",
    "        posts_reactions = {} #  post:reaction\n",
    "        post_text_div = post_src.find('div', {'class': 'feed-shared-update-v2__description-wrapper mr2'})\n",
    "\n",
    "        # if post_text_div is None:\n",
    "        #     print(post_src)\n",
    "\n",
    "        if post_text_div is not None:\n",
    "            post_text = post_text_div.find('span', {'dir': 'ltr'})\n",
    "        else:\n",
    "            post_text = None\n",
    "\n",
    "        # If post text is found\n",
    "        if post_text is not None:\n",
    "            post_text = post_text.get_text().strip()\n",
    "            #print(f'Post text: {post_text}')\n",
    "\n",
    "        reaction_cnt = post_src.find('span', {'class': 'social-details-social-counts__reactions-count'})\n",
    "\n",
    "        # If number of reactions is written as text\n",
    "        # It has different class name\n",
    "        if reaction_cnt is None:\n",
    "            reaction_cnt = post_src.find('span', {'class': 'social-details-social-counts__social-proof-text'})\n",
    "\n",
    "        if reaction_cnt is not None:\n",
    "            reaction_cnt = reaction_cnt.get_text().strip()\n",
    "            #print(f'Reactions: {reaction_cnt}')\n",
    "\n",
    "        posts_reactions[str(post_text)] = str(reaction_cnt)\n",
    "\n",
    "        all_posts_with_react.append(posts_reactions)\n",
    "\n",
    "    return all_posts_with_react"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем браузер со страницей авторизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Chrome browser\n",
    "driver = uc.Chrome(headers=HEADERS)\n",
    "\n",
    "# Opening linkedIn's login page\n",
    "# NOTE: We need to turn of 2 step authentification\n",
    "driver.get(\"https://linkedin.com/uas/login\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим учетные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waiting for the page to load\n",
    "time.sleep(random.uniform(3,7))\n",
    "\n",
    "# entering username\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "\n",
    "# Enter Your Email Address\n",
    "username.send_keys(USER_LOGIN)\n",
    "\n",
    "time.sleep(random.uniform(1,3))\n",
    "# entering password\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "\n",
    "# Enter Your Password\n",
    "pword.send_keys(USER_PASSWORD)\n",
    "\n",
    "# Clicking on the log in button\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доступных анкет на каждой странице варьируется от 2-3 до 7-8. Ограничение на парсинг 100 анкет потом релогин. Способ по снятию ограничения таймера, во второй вкладке проявлять активность: смотреть видео, проходить тесты...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat \n",
    "profile_urls = []\n",
    "\n",
    "NUM_PAGES_TO_PARSE = 50  # num pages to parsed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обходим все страницы поиска и сохраняем с них анкеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "# Iterate over pages of search results\n",
    "# to collect profile urls\n",
    "for i in range(NUM_PAGES_TO_PARSE):\n",
    "    driver.get(f'https://www.linkedin.com/search/results/people/?geoUrn=%5B%22101728296%22%5D&keywords=qa%20engineer&origin=FACETED_SEARCH&page={i+1}&sid=SJb&titleFreeText=senior')\n",
    "\n",
    "    search_result_links = driver.find_elements(By.CSS_SELECTOR, \"div.entity-result__item a.app-aware-link\")\n",
    "\n",
    "    for link in search_result_links:\n",
    "        href = link.get_attribute(\"href\")\n",
    "        if 'linkedin.com/in' in href:\n",
    "            profile_urls.append(href)\n",
    "    time.sleep(random.uniform(5,7))\n",
    "    # next_button = driver.find_element(By.CLASS_NAME,'artdeco-pagination__button--next')\n",
    "    # next_button.click()\n",
    "    print(i)\n",
    "\n",
    "profile_urls = list(set(profile_urls))\n",
    "print(len(set(profile_urls)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходим по анкетам собирая данные в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result = {}\n",
    "counter = 0\n",
    "# Parse profile urls\n",
    "for profile_url in profile_urls[:99]: # ограничение LNKD 100\n",
    "    cur_profile_url, name, works_at, experience_list, posts = get_and_print_profile_info(driver, profile_url)\n",
    "    all_profile_info = {}\n",
    "\n",
    "    all_profile_info['name'] = name\n",
    "    all_profile_info['works_at'] = works_at\n",
    "    all_profile_info['posts'] = posts\n",
    "    all_profile_info['cur_profile_url'] = cur_profile_url\n",
    "    all_profile_info['experience_list'] = experience_list\n",
    "\n",
    "    final_result[str(profile_url)] = all_profile_info\n",
    "    counter+=1\n",
    "    print(counter)\n",
    "    time.sleep(random.uniform(4,7))\n",
    "\n",
    "len(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438666"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(\"parsed_jsons\\result_final.json\", \"w\", encoding='utf-8').write(json.dumps(final_result, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the Chrome browser\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: Данные собраны, дальнейший анализ будет проведен в следующей тетрадке"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
